# Abstract Analysis

######                                                                           YANG Fei                          317106020128

## Abstract 1

​	**Recent work has shown that convolutional networks can be substantially deeper**, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output**[Background]**. In this paper, **we embrace this observation and introduce the Dense Convo- lutional Network (DenseNet)**, which connects each layer to every other layer in a feed-forward fashion**[Method]**. Whereas traditional convolutional networks with L layers have L connections—one between each layer and its subsequent layer—our network has $\frac{L(L+1)}{2}$ direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. **DenseNets have several compelling advantages:** they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage fea- ture reuse, and substantially reduce the number of parameters**[Result]**. **We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks** (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance**[Conclusion]**. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.

## Abstract 2

​	**Deeper neural networks are more difficult to train[Problem].** **We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.** We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth**[Method]**. **On the ImageNet dataset we evaluate residual nets** with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation**[Result]**.

## Abstract 3

​	**Convolutional neural networks (CNNs) are usually built by stacking convolutional operations layer-by-layer[Background].** Although CNN has shown strong capability to extract semantics from raw pixels, **its capacity to capture spatial relationships of pixels across rows and columns of an image is not fully explored[Problem].** These relationships are important to learn semantic objects with strong shape priors but weak appearance coher- ences, such as traffic lanes, which are often occluded or not even painted on the road surface as shown in Fig. 1 (a). In this paper, **we propose Spatial CNN (SCNN)**, which generalizes traditional deep layer-by-layer convolutions to slice-by- slice convolutions within feature maps, thus enabling message passings between pixels across rows and columns in a layer**[Method]**. Such SCNN is particular suitable for long continuous shape structure or large objects, with strong spatial relation- ship but less appearance clues, such as traffic lanes, poles, and wall. **We apply SCNN on a newly released very challenging traffic lane detection dataset and Cityscapse dataset1**. The re- sults show that SCNN could learn the spatial relationship for structure output and significantly improves the performance. We show that SCNN outperforms the recurrent neural net- work (RNN) based ReNet and MRF+CNN (MRFNet) in the lane detection dataset by 8.7% and 4.6% respectively. More- over, our SCNN won the 1st place on the TuSimple Bench- mark Lane Detection Challenge, with an accuracy of 96.53%**[Result]**.

## Abstract 4

​	In this paper, **we propose spatial propagation networks for learning the affinity matrix for vision tasks[Contribute].** **We show that by constructing a row/column linear propagation model,** the spatially varying transformation matrix exactly constitutes an affinity matrix that models dense, global pairwise relationships of an image. Specifically, we develop a three-way connection for the linear propagation model, which (a) formulates a sparse transformation matrix, where all elements can be the output from a deep CNN, but (b) results in a dense affinity matrix that effectively models any task-specific pairwise similarity matrix. Instead of designing the similarity kernels according to image features of two points, we can directly output all the similarities in a purely data-driven manner**[Method]**. **The spatial propagation network is a generic framework that can be applied to many affinity-related tasks,** including but not limited to image matting, segmentation and colorization, to name a few. Essentially, the model can learn semantically-aware affinity values for high-level vision tasks due to the powerful learning capability of the deep neural network classifier**[Aim]**. **We validate the framework on the task of refinement for image segmentation boundaries.** Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic segmentation tasks show that the spatial propagation network provides a general, effective and efficient solution for generating high-quality segmentation results**[Result]**.

## Abstract 5

**Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding.** Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks**[Background]**. **One central issue in this methodology is** the limited capacity of deep learning techniques to de- lineate visual objects**[Problem]**. **To solve this problem**, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling**[Aim]**. **To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks.** This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs**[Method]**. Importantly, **our system fully integrates CRF modelling with CNNs,** making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation**[Conclusion]**. **We apply the proposed method to** the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark**[Result]**.

### Reference

[1] Huang G, Liu Z, Maaten L V D, et al. Densely Connected Convolutional Networks[J]. 2016.

[2] He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.

[3] Pan X, Shi J, Luo P, et al. Spatial As Deep: Spatial CNN for Traffic Scene Understanding[J]. 2017.

[4] Liu S, De Mello S, Gu J, et al. Learning Affinity via Spatial Propagation Networks[J]. 2017.

[5] Zheng S, Jayasumana S, Romera-Paredes B, et al. Conditional Random Fields as Recurrent Neural Networks[J]. 2015:1529-1537.